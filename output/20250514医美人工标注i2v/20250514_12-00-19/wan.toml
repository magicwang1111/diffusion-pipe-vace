# This configuration should allow you to train Wan 14b t2v on 512x512x81 sized videos (or varying aspect ratios of the same size), with 24GB VRAM.

# change this
output_dir = '/home/wangxi/diffusion-pipe/output/20250514医美人工标注i2v'
# and this
dataset = '/home/wangxi/diffusion-pipe/examples/20250514医美人工标注/dataset.toml'


# training settings
epochs = 100
micro_batch_size_per_gpu = 4
pipeline_stages = 1
gradient_accumulation_steps = 1
gradient_clipping = 1
warmup_steps = 100

# eval settings
#eval_every_n_epochs = 5
#eval_before_first_step = true
#eval_micro_batch_size_per_gpu = 1
#eval_gradient_accumulation_steps = 1

# misc settings
save_every_n_epochs = 10
#checkpoint_every_n_minutes = 120
#activation_checkpointing = true
activation_checkpointing = 'unsloth'
partition_method = 'parameters'
save_dtype = 'bfloat16'
caching_batch_size = 1
steps_per_print = 1
video_clip_mode = 'single_beginning'


[model]
type = 'wan'
ckpt_path = '/mnt/data/wangxi/Wan2.1-I2V-14B-720P/'
transformer_path = '/mnt/data/shared_data/ComfyuiModels/diffusion_models/wan/wan2.1_i2v_720p_14B_bf16.safetensors'
vae_path = '/mnt/data/shared_data/ComfyuiModels/vae/wan_2.1_vae.safetensors'
llm_path = '/mnt/data/shared_data/ComfyuiModels/text_encoders/wan/umt5_xxl_fp16.safetensors'
clip_path = '/mnt/data/shared_data/ComfyuiModels/clip_vision/clip_vision_h.safetensors'
dtype = 'bfloat16'
transformer_dtype = 'float8'
timestep_sample_method = 'logit_normal'
# You can initialize the lora weights from a previously trained lora.
#init_from_existing = '/data/diffusion_pipe_training_runs/something/epoch50'

[adapter]
type = 'lora'
rank = 32
dtype = 'bfloat16'

[optimizer]
type = 'adamw_optimi'
#type = 'AdamW8bit'
lr = 2e-5
betas = [0.9, 0.99]
eps = 1e-8
weight_decay = 0.01
